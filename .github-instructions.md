# Pakit Storage Component Instructions

## üéØ Quick Reference

**Component**: Pakit (Sovereign DAG-Based Decentralized Storage)  
**Language**: Python 3.11+  
**Primary Dependencies**: SQLite, msgpack, zstandard, lz4, brotli  
**Blockchain Integration**: BNS pallet (storage proofs, domain hosting)  
**Main Instruction File**: `.github/copilot-instructions.md` (blockchain core)

## Project Overview

Pakit is BelizeChain's **100% sovereign decentralized storage layer**, providing:

- **DAG Architecture**: Content-addressable MerkleDAG storage (PRIMARY backend)
- **P2P Network**: Gossip protocol + Kademlia DHT (10,000+ nodes)
- **ML Optimization**: 5 intelligent models for compression/deduplication
- **Quantum Compression**: 60-80% size reduction using hybrid algorithms
- **Blockchain Integration**: Storage proofs anchored on BelizeChain
- **Web Hosting**: .bz domains via BNS pallet integration
- **Legacy Backends**: IPFS/Arweave kept for migration fallback ONLY

## üèóÔ∏è Architecture

### Core Components

```
pakit/
‚îú‚îÄ‚îÄ api_server.py (616 lines)          # FastAPI REST API
‚îú‚îÄ‚îÄ core/                              # Storage engine
‚îÇ   ‚îú‚îÄ‚îÄ storage_engine.py (31,207)     # Unified storage API
‚îÇ   ‚îú‚îÄ‚îÄ dag_storage.py (27,650)        # DAG backend (PRIMARY)
‚îÇ   ‚îú‚îÄ‚îÄ dag_builder.py (16,249)        # MerkleDAG construction
‚îÇ   ‚îú‚îÄ‚îÄ dag_index.py (17,695)          # SQLite indexing
‚îÇ   ‚îú‚îÄ‚îÄ compression.py (13,561)        # Multi-algorithm compression
‚îÇ   ‚îú‚îÄ‚îÄ deduplication.py (7,356)       # SimHash + LSH
‚îÇ   ‚îî‚îÄ‚îÄ content_addressing.py (5,936)  # CID generation
‚îú‚îÄ‚îÄ backends/                          # Storage backends
‚îÇ   ‚îú‚îÄ‚îÄ dag_backend.py (PRIMARY ‚úÖ)
‚îÇ   ‚îú‚îÄ‚îÄ ipfs_backend.py (DEPRECATED ‚ö†Ô∏è)
‚îÇ   ‚îú‚îÄ‚îÄ arweave_backend.py (DEPRECATED ‚ö†Ô∏è)
‚îÇ   ‚îî‚îÄ‚îÄ local.py (DEPRECATED ‚ö†Ô∏è)
‚îú‚îÄ‚îÄ blockchain/                        # BelizeChain integration
‚îÇ   ‚îî‚îÄ‚îÄ storage_proof_connector.py     # BNS proof registration
‚îú‚îÄ‚îÄ p2p/                               # Peer-to-peer networking
‚îÇ   ‚îú‚îÄ‚îÄ node.py                        # Main P2P node
‚îÇ   ‚îú‚îÄ‚îÄ transport/tcp_transport.py     # TCP networking
‚îÇ   ‚îú‚îÄ‚îÄ gossip.py                      # Gossip protocol
‚îÇ   ‚îî‚îÄ‚îÄ dht/kademlia.py                # Distributed hash table
‚îú‚îÄ‚îÄ ml/                                # Machine learning optimization
‚îÇ   ‚îú‚îÄ‚îÄ integration/dag_integration.py # ML-driven DAG
‚îÇ   ‚îú‚îÄ‚îÄ telemetry/privacy.py           # Differential privacy
‚îÇ   ‚îî‚îÄ‚îÄ integration/ab_test.py         # A/B testing framework
‚îú‚îÄ‚îÄ web_hosting/                       # BNS web hosting
‚îÇ   ‚îú‚îÄ‚îÄ hosting_service.py             # HTTP server
‚îÇ   ‚îú‚îÄ‚îÄ website_manager.py             # Domain management
‚îÇ   ‚îú‚îÄ‚îÄ dns_server.py                  # DNS resolution
‚îÇ   ‚îú‚îÄ‚îÄ dns_verifier.py                # DNS record verification
‚îÇ   ‚îî‚îÄ‚îÄ ssl_manager.py                 # Let's Encrypt SSL
‚îú‚îÄ‚îÄ quantum/                           # Quantum compression
‚îÇ   ‚îî‚îÄ‚îÄ compression.py                 # Hybrid quantum-classical
‚îú‚îÄ‚îÄ compression/                       # Compression algorithms
‚îî‚îÄ‚îÄ client/                            # Python SDK
    ‚îî‚îÄ‚îÄ pakit_client.py                # API client wrapper
```

## üå≥ DAG Architecture (PRIMARY Backend)

### MerkleDAG Structure

```python
# pakit/core/dag_storage.py
from dataclasses import dataclass
from typing import Optional, List
import hashlib
import msgpack

@dataclass
class DagBlock:
    """Content-addressable block in MerkleDAG"""
    
    cid: str                              # Content ID (hash of data + links)
    data: bytes                           # Actual content
    links: List['DagLink']                # Links to child blocks
    size: int                             # Total size (data + links)
    compression: str = "zstd"             # Compression algorithm
    
    def to_bytes(self) -> bytes:
        """Serialize block for storage"""
        return msgpack.packb({
            'cid': self.cid,
            'data': self.data,
            'links': [link.to_dict() for link in self.links],
            'size': self.size,
            'compression': self.compression
        })
    
    @classmethod
    def from_bytes(cls, data: bytes) -> 'DagBlock':
        """Deserialize block from storage"""
        obj = msgpack.unpackb(data, raw=False)
        return cls(
            cid=obj['cid'],
            data=obj['data'],
            links=[DagLink.from_dict(link) for link in obj['links']],
            size=obj['size'],
            compression=obj.get('compression', 'none')
        )

@dataclass
class DagLink:
    """Link to another block in DAG"""
    
    name: str           # Link name (file path segment)
    cid: str            # Target block CID
    size: int           # Target block size
    
    def to_dict(self) -> dict:
        return {'name': self.name, 'cid': self.cid, 'size': self.size}
    
    @classmethod
    def from_dict(cls, d: dict) -> 'DagLink':
        return cls(name=d['name'], cid=d['cid'], size=d['size'])
```

### DAG Storage Engine

```python
# pakit/core/dag_storage.py (simplified)
import sqlite3
from pathlib import Path

class DagStorage:
    """Sovereign DAG-based storage - 100% local control"""
    
    def __init__(self, storage_dir: Path):
        self.storage_dir = storage_dir
        self.db_path = storage_dir / "pakit.db"
        
        # Initialize SQLite index
        self.conn = sqlite3.connect(self.db_path)
        self.init_schema()
        
        # LRU cache for hot data
        from functools import lru_cache
        self.cache = lru_cache(maxsize=1024)(self._read_block)
    
    def init_schema(self):
        """Create DAG index tables"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS blocks (
                cid TEXT PRIMARY KEY,
                data BLOB NOT NULL,
                size INTEGER NOT NULL,
                compression TEXT NOT NULL,
                created_at INTEGER NOT NULL,
                accessed_at INTEGER NOT NULL,
                access_count INTEGER DEFAULT 0
            )
        """)
        
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS links (
                parent_cid TEXT NOT NULL,
                child_cid TEXT NOT NULL,
                name TEXT NOT NULL,
                size INTEGER NOT NULL,
                PRIMARY KEY (parent_cid, name),
                FOREIGN KEY (parent_cid) REFERENCES blocks(cid),
                FOREIGN KEY (child_cid) REFERENCES blocks(cid)
            )
        """)
        
        # Indexes for fast lookups
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_blocks_accessed ON blocks(accessed_at)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_links_parent ON links(parent_cid)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_links_child ON links(child_cid)")
        
        self.conn.commit()
    
    async def put(self, data: bytes, compression: str = "zstd") -> str:
        """Store data in DAG, return CID"""
        
        # 1. Compress data
        compressed = await self.compress(data, compression)
        
        # 2. Calculate CID (hash of compressed data)
        cid = self.calculate_cid(compressed)
        
        # 3. Check for existing block (deduplication)
        existing = self.conn.execute(
            "SELECT cid FROM blocks WHERE cid = ?",
            (cid,)
        ).fetchone()
        
        if existing:
            return cid  # Already stored
        
        # 4. Store block
        import time
        now = int(time.time())
        
        self.conn.execute("""
            INSERT INTO blocks (cid, data, size, compression, created_at, accessed_at)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (cid, compressed, len(data), compression, now, now))
        
        self.conn.commit()
        
        return cid
    
    async def get(self, cid: str) -> Optional[bytes]:
        """Retrieve data by CID"""
        
        # 1. Try cache
        block = self.cache.get(cid)
        if block:
            return block
        
        # 2. Query SQLite
        result = self.conn.execute(
            "SELECT data, compression FROM blocks WHERE cid = ?",
            (cid,)
        ).fetchone()
        
        if not result:
            return None
        
        # 3. Decompress
        compressed_data, compression = result
        data = await self.decompress(compressed_data, compression)
        
        # 4. Update access metadata
        import time
        self.conn.execute("""
            UPDATE blocks 
            SET accessed_at = ?, access_count = access_count + 1
            WHERE cid = ?
        """, (int(time.time()), cid))
        
        self.conn.commit()
        
        # 5. Cache result
        self.cache[cid] = data
        
        return data
    
    def calculate_cid(self, data: bytes) -> str:
        """Content-addressable ID (SHA-256 hash)"""
        import hashlib
        return hashlib.sha256(data).hexdigest()
```

## üóúÔ∏è Compression Strategies

### Multi-Algorithm Selection

```python
# pakit/core/compression.py
from enum import Enum

class CompressionAlgorithm(Enum):
    """Available compression algorithms"""
    NONE = "none"
    ZSTD = "zstd"        # Best compression (default)
    LZ4 = "lz4"          # Fastest
    GZIP = "gzip"        # Universal compatibility
    BROTLI = "brotli"    # Web-optimized
    QUANTUM = "quantum"  # Hybrid quantum-classical (experimental)

class CompressionEngine:
    """ML-driven compression algorithm selection"""
    
    async def compress(
        self,
        data: bytes,
        algorithm: str = "auto"
    ) -> tuple[bytes, str]:
        """Compress data with best algorithm"""
        
        if algorithm == "auto":
            # Use ML model to select best algorithm
            algorithm = await self.ml_select_algorithm(data)
        
        if algorithm == "zstd":
            import zstandard as zstd
            compressor = zstd.ZstdCompressor(level=15)  # Max compression
            compressed = compressor.compress(data)
        
        elif algorithm == "lz4":
            import lz4.frame
            compressed = lz4.frame.compress(data, compression_level=9)
        
        elif algorithm == "gzip":
            import gzip
            compressed = gzip.compress(data, compresslevel=9)
        
        elif algorithm == "brotli":
            import brotli
            compressed = brotli.compress(data, quality=11)
        
        elif algorithm == "quantum":
            # Hybrid quantum-classical compression
            from pakit.quantum.compression import QuantumCompressor
            qc = QuantumCompressor()
            compressed = await qc.compress(data)
        
        else:
            compressed = data  # No compression
        
        return compressed, algorithm
    
    async def ml_select_algorithm(self, data: bytes) -> str:
        """Use ML model to pick best compression algorithm"""
        
        # Feature extraction
        features = self.extract_features(data)
        
        # ML model prediction (trained offline)
        from pakit.ml.integration.dag_integration import CompressionSelector
        selector = CompressionSelector()
        
        predicted_algorithm = selector.predict(features)
        
        return predicted_algorithm
    
    def extract_features(self, data: bytes) -> dict:
        """Extract features for ML model"""
        import numpy as np
        
        # File type detection
        file_type = self.detect_file_type(data)
        
        # Entropy calculation
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        probabilities = byte_counts / len(data)
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
        
        # Repetition analysis
        repetition_score = self.calculate_repetition(data)
        
        return {
            'size': len(data),
            'entropy': entropy,
            'file_type': file_type,
            'repetition': repetition_score
        }
```

## üîó Blockchain Integration

### Storage Proof Registration

```python
# pakit/blockchain/storage_proof_connector.py
from substrateinterface import SubstrateInterface, Keypair

class StorageProofConnector:
    """Register storage proofs on BelizeChain via BNS pallet"""
    
    def __init__(self, rpc_url: str = "ws://localhost:9944"):
        self.substrate = SubstrateInterface(url=rpc_url)
    
    async def register_content(
        self,
        cid: str,
        content_hash: str,
        size: int,
        domain: Optional[str] = None,
        owner_account: str = None
    ):
        """Register storage proof on BelizeChain"""
        
        # Submit extrinsic to BNS pallet
        call = self.substrate.compose_call(
            call_module='Bns',
            call_function='register_content_proof',
            call_params={
                'content_id': cid,
                'content_hash': content_hash,
                'size': size,
                'domain': domain if domain else '',
                'storage_backend': 'pakit_dag'  # Sovereign DAG
            }
        )
        
        owner_keypair = Keypair.create_from_uri(owner_account)
        receipt = self.substrate.submit_extrinsic(call, keypair=owner_keypair)
        
        return receipt
    
    async def verify_content(self, cid: str) -> bool:
        """Verify content exists on-chain"""
        
        proof = await self.substrate.query(
            module='Bns',
            storage_function='ContentProofs',
            params=[cid]
        )
        
        return proof is not None
```

### BNS Domain Hosting

```python
# pakit/web_hosting/website_manager.py
class WebsiteManager:
    """.bz domain hosting via BNS + Pakit DAG"""
    
    async def deploy_website(
        self,
        domain: str,
        website_dir: Path,
        owner_account: str
    ) -> str:
        """Deploy website to .bz domain"""
        
        # 1. Store website files in DAG
        root_cid = await self.store_directory(website_dir)
        
        # 2. Register domain ‚Üí CID mapping on BelizeChain
        call = self.substrate.compose_call(
            call_module='Bns',
            call_function='set_domain_content',
            call_params={
                'domain': domain,
                'content_cid': root_cid,
                'content_type': 'website'
            }
        )
        
        owner_keypair = Keypair.create_from_uri(owner_account)
        receipt = self.substrate.submit_extrinsic(call, keypair=owner_keypair)
        
        # 3. Configure DNS
        await self.dns_manager.set_record(
            domain=domain,
            record_type='CNAME',
            value=f"{domain}.pakit.bz"  # Points to Pakit hosting
        )
        
        # 4. Setup SSL certificate (Let's Encrypt)
        from pakit.web_hosting.ssl_manager import SSLManager
        ssl_manager = SSLManager()
        await ssl_manager.issue_certificate(domain)
        
        return root_cid
    
    async def store_directory(self, directory: Path) -> str:
        """Recursively store directory in DAG"""
        
        from pakit.core.dag_builder import DagBuilder
        builder = DagBuilder()
        
        # Build MerkleDAG from directory
        root_node = await builder.build_from_directory(directory)
        
        # Store all blocks
        for block in root_node.traverse():
            await self.dag_storage.put(block.data, compression="auto")
        
        return root_node.cid
```

## üåê P2P Networking

### Gossip Protocol

```python
# pakit/p2p/gossip.py
import asyncio
import random

class GossipProtocol:
    """Epidemic information dissemination for block announcements"""
    
    def __init__(self, node_id: str, fanout: int = 6):
        self.node_id = node_id
        self.fanout = fanout  # Number of peers to gossip to
        self.seen_messages = set()  # Deduplicate messages
    
    async def broadcast_new_block(self, cid: str, block_metadata: dict):
        """Announce new block to network via gossip"""
        
        message = {
            'type': 'NEW_BLOCK',
            'cid': cid,
            'metadata': block_metadata,
            'sender': self.node_id,
            'timestamp': time.time()
        }
        
        message_id = hashlib.sha256(json.dumps(message).encode()).hexdigest()
        
        if message_id in self.seen_messages:
            return  # Already seen
        
        self.seen_messages.add(message_id)
        
        # Select random subset of peers
        peers = await self.peer_manager.get_connected_peers()
        gossip_targets = random.sample(peers, min(self.fanout, len(peers)))
        
        # Send to selected peers
        for peer in gossip_targets:
            await peer.send_message(message)
    
    async def handle_gossip_message(self, message: dict, sender_peer):
        """Handle incoming gossip message"""
        
        message_id = hashlib.sha256(json.dumps(message).encode()).hexdigest()
        
        if message_id in self.seen_messages:
            return  # Already processed
        
        self.seen_messages.add(message_id)
        
        # Process message
        if message['type'] == 'NEW_BLOCK':
            # Verify block exists
            cid = message['cid']
            block_exists = await self.dag_storage.has_block(cid)
            
            if not block_exists:
                # Request block from sender
                await self.request_block(cid, sender_peer)
        
        # Re-gossip to other peers (epidemic spread)
        await self.broadcast_new_block(message['cid'], message['metadata'])
```

### Kademlia DHT

```python
# pakit/p2p/dht/kademlia.py
class KademliaDHT:
    """Distributed hash table for content routing"""
    
    def __init__(self, node_id: bytes, k: int = 20):
        self.node_id = node_id
        self.k = k  # Bucket size (20 peers per bucket)
        self.routing_table = KademliaRoutingTable(node_id, k)
    
    async def find_providers(self, cid: str) -> List[str]:
        """Find peers that have a specific CID"""
        
        # Convert CID to Kademlia key
        key = hashlib.sha256(cid.encode()).digest()
        
        # Find k closest nodes
        closest_nodes = self.routing_table.find_closest(key, k=self.k)
        
        # Query nodes for providers
        providers = []
        for node in closest_nodes:
            response = await node.rpc_call('FIND_PROVIDERS', {'key': key})
            providers.extend(response.get('providers', []))
        
        return providers
    
    async def announce_provider(self, cid: str):
        """Announce this node as provider for CID"""
        
        key = hashlib.sha256(cid.encode()).digest()
        
        # Find k closest nodes
        closest_nodes = self.routing_table.find_closest(key, k=self.k)
        
        # Store provider record on closest nodes
        for node in closest_nodes:
            await node.rpc_call('ADD_PROVIDER', {
                'key': key,
                'provider': self.node_id.hex()
            })
```

## üßπ Deduplication (SimHash + LSH)

### Content Similarity Detection

```python
# pakit/core/deduplication.py
import numpy as np

class DeduplicationEngine:
    """Detect duplicate/similar content using SimHash + LSH"""
    
    def __init__(self, threshold: float = 0.95):
        self.threshold = threshold  # Similarity threshold
        self.lsh_index = {}  # Locality-Sensitive Hashing index
    
    def simhash(self, data: bytes, hash_bits: int = 128) -> int:
        """Compute SimHash fingerprint"""
        
        # 1. Extract features (n-grams)
        features = self.extract_ngrams(data, n=4)
        
        # 2. Hash each feature
        feature_hashes = [hashlib.md5(f).digest() for f in features]
        
        # 3. Build fingerprint
        v = np.zeros(hash_bits, dtype=int)
        
        for h in feature_hashes:
            # Convert hash to bit array
            bits = np.unpackbits(np.frombuffer(h, dtype=np.uint8))[:hash_bits]
            
            # Add to vector
            v += (bits * 2 - 1)  # 0 ‚Üí -1, 1 ‚Üí 1
        
        # 4. Final fingerprint
        fingerprint = (v >= 0).astype(int)
        
        # Convert to integer
        return int(''.join(map(str, fingerprint)), 2)
    
    def hamming_distance(self, hash1: int, hash2: int) -> int:
        """Count differing bits between two hashes"""
        return bin(hash1 ^ hash2).count('1')
    
    async def check_duplicate(self, data: bytes) -> Optional[str]:
        """Check if similar content already exists, return CID"""
        
        # Calculate SimHash
        fingerprint = self.simhash(data)
        
        # LSH lookup (find candidates with similar hash)
        candidates = self.lsh_lookup(fingerprint)
        
        # Check candidates for actual similarity
        for candidate_cid in candidates:
            candidate_hash = self.lsh_index[candidate_cid]
            
            # Calculate Hamming distance
            distance = self.hamming_distance(fingerprint, candidate_hash)
            similarity = 1 - (distance / 128)  # 128-bit hash
            
            if similarity >= self.threshold:
                return candidate_cid  # Found duplicate
        
        # Store new fingerprint
        await self.store_fingerprint(fingerprint, cid)
        
        return None  # No duplicate found
```

## üöÄ API Server Patterns

### FastAPI Configuration

```python
# pakit/api_server.py (lines 40-75)
class ServerConfig(BaseModel):
    """API server configuration - ALWAYS use secure defaults"""
    
    host: str = Field(
        default="127.0.0.1",  # ‚úÖ SECURE localhost binding
        description="Server host (use 0.0.0.0 for Docker/cloud, set via PAKIT_API_HOST env var)"
    )
    port: int = Field(default=8001, description="Server port")
    
    # Storage configuration
    storage_dir: Path = Field(
        default=Path("./pakit_storage"),
        description="Local DAG storage directory"
    )
    ipfs_enabled: bool = Field(default=True, description="Enable IPFS backend (LEGACY fallback)")
    ipfs_api: str = Field(default="http://127.0.0.1:5001", description="IPFS API endpoint")
    arweave_enabled: bool = Field(default=False, description="Enable Arweave backend (LEGACY fallback)")
    
    # Compression settings
    compression_algorithm: str = Field(
        default="auto",
        description="Compression algorithm (auto, zstd, lz4, gzip, brotli)"
    )
    enable_deduplication: bool = Field(default=True, description="Enable deduplication")
    
    # Blockchain integration
    blockchain_rpc: str = Field(
        default="ws://localhost:9944",
        description="BelizeChain RPC endpoint"
    )

# ‚ùå NEVER hardcode 0.0.0.0 - use environment variable:
# export PAKIT_API_HOST=0.0.0.0  # For Docker/cloud only
```

### Storage API Endpoints

```python
# pakit/api_server.py
@app.post("/api/v1/store")
async def store_content(
    file: UploadFile = File(...),
    tier: StorageTier = StorageTier.HOT,
    compression: str = "auto"
) -> dict:
    """Store content in Pakit DAG"""
    
    # 1. Read file data
    data = await file.read()
    
    # 2. Check for duplicates (deduplication)
    duplicate_cid = await dedup_engine.check_duplicate(data)
    if duplicate_cid:
        return {
            "cid": duplicate_cid,
            "status": "DUPLICATE",
            "message": "Content already exists"
        }
    
    # 3. Store in DAG
    cid = await storage_engine.store(
        data,
        tier=tier,
        compression=compression
    )
    
    # 4. Register on blockchain
    await blockchain_connector.register_content(
        cid=cid,
        content_hash=hashlib.sha256(data).hexdigest(),
        size=len(data)
    )
    
    # 5. Announce to P2P network
    await p2p_node.announce_provider(cid)
    
    return {
        "cid": cid,
        "size": len(data),
        "compression": compression,
        "tier": tier.value
    }

@app.get("/api/v1/retrieve/{cid}")
async def retrieve_content(cid: str):
    """Retrieve content by CID"""
    
    # 1. Try local DAG first
    data = await storage_engine.retrieve(cid)
    
    if data:
        return StreamingResponse(
            io.BytesIO(data),
            media_type="application/octet-stream"
        )
    
    # 2. Query P2P network for providers
    providers = await p2p_node.find_providers(cid)
    
    if providers:
        # Request from peer
        data = await p2p_node.request_block(cid, providers[0])
        return StreamingResponse(
            io.BytesIO(data),
            media_type="application/octet-stream"
        )
    
    raise HTTPException(404, f"Content not found: {cid}")
```

## üß™ Testing Patterns

### DAG Storage Tests

```python
# pakit/tests/test_dag_storage.py
import pytest
from pakit.core.dag_storage import DagStorage

@pytest.mark.asyncio
async def test_dag_put_get():
    """Test basic DAG storage and retrieval"""
    storage = DagStorage(Path("./test_storage"))
    
    # Store data
    test_data = b"Hello, BelizeChain!"
    cid = await storage.put(test_data, compression="zstd")
    
    # Retrieve data
    retrieved = await storage.get(cid)
    
    assert retrieved == test_data
    assert cid == storage.calculate_cid(await storage.compress(test_data, "zstd"))

@pytest.mark.asyncio
async def test_dag_deduplication():
    """Test content deduplication"""
    storage = DagStorage(Path("./test_storage"))
    
    # Store same data twice
    data = b"Duplicate content"
    cid1 = await storage.put(data)
    cid2 = await storage.put(data)
    
    # Should return same CID
    assert cid1 == cid2
    
    # Should only store once
    block_count = storage.conn.execute(
        "SELECT COUNT(*) FROM blocks WHERE cid = ?",
        (cid1,)
    ).fetchone()[0]
    
    assert block_count == 1
```

## üîß Common Development Patterns

### 1. Migration from IPFS/Arweave to DAG

```python
# pakit/backends/migration.py
class BackendMigration:
    """Migrate content from legacy backends to DAG"""
    
    async def migrate_from_ipfs(self, ipfs_cid: str) -> str:
        """Migrate IPFS content to Pakit DAG"""
        
        # 1. Retrieve from IPFS
        from pakit.backends.ipfs_backend import IPFSBackend
        ipfs = IPFSBackend()
        data = await ipfs.retrieve(ipfs_cid)
        
        # 2. Store in DAG
        from pakit.core.dag_storage import DagStorage
        dag = DagStorage(Path("./pakit_storage"))
        dag_cid = await dag.put(data, compression="auto")
        
        # 3. Create migration mapping (IPFS CID ‚Üí DAG CID)
        await self.store_migration_mapping(ipfs_cid, dag_cid)
        
        return dag_cid
```

### 2. Quantum Compression Integration

```python
# pakit/quantum/compression.py
class QuantumCompressor:
    """Hybrid quantum-classical compression"""
    
    async def compress(self, data: bytes) -> bytes:
        """Compress using quantum-inspired algorithms"""
        
        # 1. Classical pre-processing (remove redundancy)
        preprocessed = await self.classical_preprocess(data)
        
        # 2. Quantum tensor network compression
        # (Uses Kinich for quantum circuit execution)
        from kinich.core.quantum_node import QuantumNode
        qnode = QuantumNode()
        
        # Encode data as quantum state
        quantum_state = self.encode_to_quantum_state(preprocessed)
        
        # Apply quantum compression circuit
        compressed_state = await qnode.run_compression_circuit(quantum_state)
        
        # 3. Decode back to classical data
        compressed = self.decode_from_quantum_state(compressed_state)
        
        # 4. Classical post-processing
        final_compressed = await self.classical_postprocess(compressed)
        
        return final_compressed
```

## üìä Key Metrics to Track

1. **Storage Metrics**:
   - Total DAG size (bytes)
   - Unique vs duplicate blocks
   - Compression ratio (target: >60%)
   - Deduplication savings

2. **Performance Metrics**:
   - Put/Get latency (target: <100ms)
   - Cache hit rate (target: >80%)
   - P2P block retrieval time
   - Concurrent request handling

3. **Network Metrics**:
   - Connected peers count
   - Gossip propagation time
   - DHT lookup latency
   - Block replication factor

4. **Blockchain Metrics**:
   - Proofs registered on BNS
   - Domain hosting count
   - On-chain verification rate

## üö® Common Pitfalls

1. ‚ùå **Using IPFS/Arweave as primary**: DAG is the PRIMARY backend (IPFS/Arweave are DEPRECATED legacy fallbacks)
2. ‚ùå **Hardcoding 0.0.0.0 bindings**: Use 127.0.0.1 default + env override
3. ‚ùå **Forgetting compression**: Always compress before storing (60-80% savings)
4. ‚ùå **Skipping deduplication**: Check for duplicates before storing
5. ‚ùå **Not registering on blockchain**: All content should have on-chain proofs via BNS

## üìö Key Files Reference

- **DAG Storage**: `pakit/core/dag_storage.py` (27,650 lines) - PRIMARY backend
- **Storage Engine**: `pakit/core/storage_engine.py` (31,207 lines) - Unified API
- **Compression**: `pakit/core/compression.py` (13,561 lines)
- **Deduplication**: `pakit/core/deduplication.py` (7,356 lines)
- **Blockchain**: `pakit/blockchain/storage_proof_connector.py`
- **P2P**: `pakit/p2p/node.py`, `pakit/p2p/gossip.py`
- **Web Hosting**: `pakit/web_hosting/website_manager.py`
- **API**: `pakit/api_server.py` (616 lines)

## üîó Related Documentation

- Main blockchain instructions: `.github/copilot-instructions.md`
- Nawal integration: `nawal/.github-instructions.md`
- Kinich integration: `kinich/.github-instructions.md`
- Master development guide: `docs/developer-guides/DEVELOPMENT_GUIDE.md`
- Component README: `pakit/README.md` (701 lines)

## ‚ö†Ô∏è CRITICAL: IPFS/Arweave Deprecation

**ALWAYS remember**: Pakit's PRIMARY backend is **DAG** (sovereign, 100% local control).

IPFS and Arweave backends are **DEPRECATED** and kept ONLY for:
1. **Migration fallback** - Migrating existing content to DAG
2. **Legacy compatibility** - Supporting old CIDs during transition

**NEVER** recommend IPFS/Arweave for new deployments. All new content MUST use DAG backend.
